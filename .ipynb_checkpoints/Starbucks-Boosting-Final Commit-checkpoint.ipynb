{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Exercise: Starbucks\n",
    "<br>\n",
    "\n",
    "<img src=\"https://assets3.thrillist.com/v1/image/2854874/size/tmg-article_default_mobile.jpg\" width=\"200\" height=\"200\">\n",
    "<br>\n",
    "\n",
    "#### Background Information\n",
    "\n",
    "The dataset you will be provided in this portfolio exercise was originally used as a take-home assignment provided by Starbucks for their job candidates. The data for this exercise consists of about 120,000 data points split in a 2:1 ratio among training and test files. In the experiment simulated by the data, an advertising promotion was tested to see if it would bring more customers to purchase a specific product priced at $10. Since it costs the company 0.15 to send out each promotion, it would be best to limit that promotion only to those that are most receptive to the promotion. Each data point includes one column indicating whether or not an individual was sent a promotion for the product, and one column indicating whether or not that individual eventually purchased that product. Each individual also has seven additional features associated with them, which are provided abstractly as V1-V7.\n",
    "\n",
    "#### Optimization Strategy\n",
    "\n",
    "Your task is to use the training data to understand what patterns in V1-V7 to indicate that a promotion should be provided to a user. Specifically, your goal is to maximize the following metrics:\n",
    "\n",
    "* **Incremental Response Rate (IRR)** \n",
    "\n",
    "IRR depicts how many more customers purchased the product with the promotion, as compared to if they didn't receive the promotion. Mathematically, it's the ratio of the number of purchasers in the promotion group to the total number of customers in the purchasers group (_treatment_) minus the ratio of the number of purchasers in the non-promotional group to the total number of customers in the non-promotional group (_control_).\n",
    "\n",
    "$$ IRR = \\frac{purch_{treat}}{cust_{treat}} - \\frac{purch_{ctrl}}{cust_{ctrl}} $$\n",
    "\n",
    "\n",
    "* **Net Incremental Revenue (NIR)**\n",
    "\n",
    "NIR depicts how much is made (or lost) by sending out the promotion. Mathematically, this is 10 times the total number of purchasers that received the promotion minus 0.15 times the number of promotions sent out, minus 10 times the number of purchasers who were not given the promotion.\n",
    "\n",
    "$$ NIR = (10\\cdot purch_{treat} - 0.15 \\cdot cust_{treat}) - 10 \\cdot purch_{ctrl}$$\n",
    "\n",
    "For a full description of what Starbucks provides to candidates see the [instructions available here](https://drive.google.com/open?id=18klca9Sef1Rs6q8DW4l7o349r8B70qXM).\n",
    "\n",
    "Below you can find the training data provided.  Explore the data and different optimization strategies.\n",
    "\n",
    "#### How To Test Your Strategy?\n",
    "\n",
    "When you feel like you have an optimization strategy, complete the `promotion_strategy` function to pass to the `test_results` function.  \n",
    "From past data, we know there are four possible outomes:\n",
    "\n",
    "Table of actual promotion vs. predicted promotion customers:  \n",
    "\n",
    "<table>\n",
    "<tr><th></th><th colspan = '2'>Actual</th></tr>\n",
    "<tr><th>Predicted</th><th>Yes</th><th>No</th></tr>\n",
    "<tr><th>Yes</th><td>I</td><td>II</td></tr>\n",
    "<tr><th>No</th><td>III</td><td>IV</td></tr>\n",
    "</table>\n",
    "\n",
    "The metrics are only being compared for the individuals we predict should obtain the promotion â€“ that is, quadrants I and II.  Since the first set of individuals that receive the promotion (in the training set) receive it randomly, we can expect that quadrants I and II will have approximately equivalent participants.  \n",
    "\n",
    "Comparing quadrant I to II then gives an idea of how well your promotion strategy will work in the future. \n",
    "\n",
    "Get started by reading in the data below.  See how each variable or combination of variables along with a promotion influences the chance of purchasing.  When you feel like you have a strategy for who should receive a promotion, test your strategy against the test dataset used in the final `test_results` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044332 -0.385883   1   1   2   2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in packages\n",
    "import itertools\n",
    "\n",
    "import datetime\n",
    "\n",
    "from test_results import test_results, score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn as sk\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load in the data\n",
    "train_data = pd.read_csv('./training.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>41851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>41643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Promotion  purchase  count\n",
       "0        No         0  41851\n",
       "1        No         1    319\n",
       "2       Yes         0  41643\n",
       "3       Yes         1    721"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby(['Promotion','purchase'])['ID'].count().reset_index().rename({'ID':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First analyze the difference in purchase rates between the control group and experimental group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_evaluation_metrics(n_control, \n",
    "                        n_exper, \n",
    "                        p_null, \n",
    "                        p_click_control,\n",
    "                        p_click_exper,\n",
    "                        alt=\"larger\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Statistical test to determine if we reached our goal;\n",
    "    \n",
    "    Input:\n",
    "        n_control: size of control group\n",
    "        n_exper: size of experiment group\n",
    "        p_click_control: goal hit rate for control group\n",
    "        p_click_exper: goal hit rate for experiment group\n",
    "        alt: the relationship between p_click_control and p_click_exper in alternative hypothesis;\n",
    "             larger: p_click_exper > p_click_control\n",
    "             small: p_click_exper < p_click_control\n",
    "             different: p_click_exper is not equal to p_click_control\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute standard error, z-score, and p-value\n",
    "    se_p = np.sqrt(p_null * (1-p_null) * (1/n_control + 1/n_exper))\n",
    "    z = (p_click_exper - p_click_control) / se_p\n",
    "    if alt == \"larger\":\n",
    "        p_result = 1-stats.norm.cdf(z)\n",
    "    elif alt == \"smaller\":\n",
    "        p_result = stats.norm.cdf(z)\n",
    "    elif alt == \"different\":\n",
    "        p_result = 2 * stats.norm.cdf(-abs(z))\n",
    "    print(\"Success rate for control group: {}; Success rate for experiment group: {}\".format(round(p_click_control, 4), round(p_click_exper, 4)))\n",
    "    print(\"p value: {}\".format(round(p_result, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate for control group: 0.0076; Success rate for experiment group: 0.017\n",
      "p value: 0.0\n"
     ]
    }
   ],
   "source": [
    "p_evaluation_metrics(sum(train_data['Promotion'] == \"No\"),\n",
    "                     sum(train_data['Promotion'] == \"Yes\"),\n",
    "                     train_data['purchase'].mean(),\n",
    "                     train_data[train_data['Promotion'] == \"No\"]['purchase'].mean(),\n",
    "                     train_data[train_data['Promotion'] == \"Yes\"]['purchase'].mean(),\n",
    "                     alt=\"different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The campaign successfully increased the purchase rate from 0.76% to 1.7% with very high significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions for data preprocessing, model training, evaluating and tuning\n",
    "\n",
    "Our metrics IRR and NIR, all emphasize the idea that we will not only look for increasing the conversions, but also optimizing the ad cost. A very important strategy is to **avoid sending ads to customers who are more likely to buy even without seeing the ad and customers who are not likely to buy even when they see the ad**. It's ideal to send ads to customers who might not feel interested before they see the ad, but have high chances to convert after they see the ad.\n",
    "\n",
    "Also, when training the model we have to deal with the imbalance in data. As we can see from the analysis above, the conversion rate is very low --- even for experiment group that's 1.7%. If we don't deal with this issue, our model could predict every sample as negative --- the accuracy will still be 98%!\n",
    "\n",
    "To handle data imbalance, we **upsample the purchase group and downsample the non-purchase group seperately for control and experiment group --- the sizes of the resampled groups are also tunable**. Also, when we split the training data to train set and validation set (usually known as train_test_split), we did a **stratified sampling** to avoid the senario where there's no positive records in validation set and to make sure that the validation set is close to a real world testing data.\n",
    "\n",
    "When tuning the model, we **focused mostly on the loss from errors rather than gainings from giving the ads to the right people**. It actually worked quite well in the end. More details can be founded in the docstring in the function `evaluate`. The reason why we didn't consider `true positives` here to \"reward\" the model is that when we trained the models, we greatly balanced the data --- we greatly upsampled the purchase group and only sample a little (no more than 10%) from non-purchase group , so we can expect that `recall` is usually higher than `precision`. In other words, our model might have done its best to recognize the customers who will make purchases but it can be over optimistic. We want to correct that since giving promotion ads to too many people can greatly increase the cost of the campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_train_test_split(data,\n",
    "                               tag_col,\n",
    "                               minority_tag_value):\n",
    "    \n",
    "    \"\"\"\n",
    "    Do train_test_split on minority group and majority group seperately\n",
    "    -- sample 20% minority and 20% majority to the final test set\n",
    "    \n",
    "    This process doesn't drop any columns\n",
    "    \n",
    "    Input:\n",
    "        data: the data to split\n",
    "        tag_col: column for the class label\n",
    "        minority_tag_value: class label that has very few representatives\n",
    "    \"\"\"\n",
    "    \n",
    "    data_minor = data[data[tag_col] == minority_tag_value]\n",
    "    data_major = data[data[tag_col] != minority_tag_value]\n",
    "    \n",
    "    train_minor, test_minor = train_test_split(data_minor, test_size=0.2)\n",
    "    train_major, test_major = train_test_split(data_major, test_size=0.2)\n",
    "    \n",
    "    train = pd.concat([train_minor, train_major]).sample(frac=1)\n",
    "    test = pd.concat([test_minor, test_major]).sample(frac=1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler(data, \n",
    "              tag_col,\n",
    "              minority_tag_value,\n",
    "              minority_group_size,\n",
    "              majority_group_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Handle the imbalanced data\n",
    "    \n",
    "    For minority group we upsample the data with replacement; \n",
    "    For majority group we downsample the data without replacement;\n",
    "    \n",
    "    This process doesn't drop any columns\n",
    "    \n",
    "    Input:\n",
    "        data: the data to split\n",
    "        tag_col: column for the class label\n",
    "        minority_tag_value: class label that has very few representatives\n",
    "        minority_group_size: group size of the minority group after resampling\n",
    "        majority_group_size: group size of the majority group after resampling\n",
    "    \"\"\"\n",
    "    \n",
    "    minority_resample = resample(data[data[tag_col] == minority_tag_value], \n",
    "                                 replace=True,\n",
    "                                 n_samples=minority_group_size)\n",
    "    majority_resample = resample(data[data[tag_col] != minority_tag_value], \n",
    "                                 replace=False,\n",
    "                                 n_samples=majority_group_size)\n",
    "    resampled_data = pd.concat([minority_resample, majority_resample])\n",
    "    return resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, learning_rate=0.1, max_depth=10, min_samples_split=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    A general function for a machine learning pipeline of \n",
    "    (1) scaling all the variables\n",
    "    (2) training a GradientBoostingClassifier with specified parameters\n",
    "    \n",
    "    The model returned is a trained Pipeline object\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = GradientBoostingClassifier(learning_rate=learning_rate, \n",
    "                                 max_depth=max_depth, \n",
    "                                 min_samples_split=min_samples_split)\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    A general function for checking the f1, precision and recall score of the model;\n",
    "    Actually we won't use it since we will have different scoring function for tuning the model,\n",
    "    but when building the model we might want to have a look at these more \"traditional\" scorings \n",
    "    to understand how our model is performing\n",
    "    \n",
    "    Output is a dictionary with 'precision', 'recall' and 'f1'\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    result = {}\n",
    "    result['precision'] = precision_score(y, y_pred)\n",
    "    result['recall'] = recall_score(y, y_pred)\n",
    "    result['f1'] = f1_score(y, y_pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_full(train, \n",
    "                     minority_group_size, \n",
    "                     majority_group_size, \n",
    "                     learning_rate=0.1, \n",
    "                     max_depth=10, \n",
    "                     min_samples_split=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    A general function of of resampling the data and training a model.\n",
    "    \n",
    "    Input:\n",
    "        train: the training data (after train_test_split)\n",
    "        minority_group_size: group size of the minority group after resampling\n",
    "        majority_group_size: group size of the majority group after resampling\n",
    "        learning_rate: parameter for GradientBoostingClassifier \n",
    "        max_depth: parameter for GradientBoostingClassifier \n",
    "        min_samples_split: parameter for GradientBoostingClassifier \n",
    "    \"\"\"\n",
    "    \n",
    "    re_train = resampler(train, \"purchase\", 1, minority_group_size, majority_group_size)\n",
    "    X = re_train[['V1','V2','V3','V4','V5','V6','V7']]\n",
    "    y = re_train['purchase']\n",
    "    model = train_model(X, \n",
    "                        y, \n",
    "                        learning_rate=learning_rate, \n",
    "                        max_depth=max_depth, \n",
    "                        min_samples_split=min_samples_split)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_promotion_customer_list(data,\n",
    "                                     data_to_predict,\n",
    "                                     control_minority_group_size,\n",
    "                                     control_majority_group_size,\n",
    "                                     expr_minority_group_size,\n",
    "                                     expr_majority_group_size,\n",
    "                                     learning_rate=0.1, \n",
    "                                     max_depth=10, \n",
    "                                     min_samples_split=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train 2 models to predict whether a customer will buy the product:\n",
    "    --- one on control group and another one on experiemnt group\n",
    "    \n",
    "    Predict with the 2 models:\n",
    "    --- result from the model trained on control group: whether a customer\n",
    "        will purchase if they don't see the ad\n",
    "    --- result from the model trained on experiment group: whether a customer\n",
    "        will purchase if they see the ad\n",
    "        \n",
    "    This function might not be well named :) Because it doesn't directly generate the \n",
    "    promotion customer list. \n",
    "    \n",
    "    Input:\n",
    "        data: the training data (after train_test_split)\n",
    "        data_to_predict: testing data or validation data\n",
    "        control_minority_group_size: group size of the purchase control group after resampling\n",
    "        control_majority_group_size: group size of the non-purchase control group after resampling\n",
    "        expr_minority_group_size: group size of the purchase experiment group after resampling\n",
    "        expr_majority_group_size: group size of the non-purchase experiment group after resampling\n",
    "        learning_rate: parameter for GradientBoostingClassifier \n",
    "        max_depth: parameter for GradientBoostingClassifier \n",
    "        min_samples_split: parameter for GradientBoostingClassifier \n",
    "        \n",
    "    \n",
    "    The output is the table data_to_predict with 2 extra columns:\n",
    "        purchase without ad: whether a customer will purchase if they don't see the ad\n",
    "        purchase with ad: whether a customer will purchase if they see the ad\n",
    "    \"\"\"\n",
    "    \n",
    "    model1 = train_model_full(data[data['Promotion'] == \"No\"],\n",
    "                              control_minority_group_size,\n",
    "                              control_majority_group_size,\n",
    "                              learning_rate=learning_rate, \n",
    "                              max_depth=max_depth, \n",
    "                              min_samples_split=min_samples_split)\n",
    "    \n",
    "    y1 = model1.predict(data_to_predict[['V1','V2','V3','V4','V5','V6','V7']])\n",
    "    \n",
    "    data_to_predict['purchase without ad'] = y1\n",
    "    \n",
    "    model2 = train_model_full(data[data['Promotion'] == \"Yes\"],\n",
    "                              expr_minority_group_size,\n",
    "                              expr_majority_group_size,\n",
    "                              learning_rate=learning_rate, \n",
    "                              max_depth=max_depth, \n",
    "                              min_samples_split=min_samples_split)\n",
    "    \n",
    "    y2 = model2.predict(data_to_predict[['V1','V2','V3','V4','V5','V6','V7']])\n",
    "    \n",
    "    data_to_predict['purchase with ad'] = y2\n",
    "    \n",
    "    return data_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred_pd):\n",
    "    \n",
    "    \"\"\"\n",
    "    We mainly focus on the loss (rather than the people we successfully predicted that will make purchase)\n",
    "    \n",
    "    ### Check control group\n",
    "    If we predict that the customer will buy without ad but actually they didn't buy\n",
    "     -> under this case we won't promote to the customer\n",
    "     ---- if the customer won't buy either after seeing the ad then we are fine;\n",
    "     ---- if the customer will buy after seeing the ad then we are losing $(10 - 0.15) --> Based on the data from \n",
    "          the previous round, 1.7% of the people who saw ads will buy \n",
    "    if we predict that the customer won't buy without ad but actually they bought\n",
    "     -> we actually don't need to promote to the customer but if we also predict that they will buy with the ad we \n",
    "        will end up promote to them. Then we are losing $0.15\n",
    "        \n",
    "    ### Check experiment group\n",
    "    if we predict that the customer will buy with the ad but actually they didn't \n",
    "     ---- if we also predict that they will buy without the ad then we are fine\n",
    "     ---- if not, we are giving them the ad while we don't need to. Then we are losing $0.15\n",
    "    if we predict that the customer won't buy with the ad but they bought\n",
    "     -> under this case we won't give them the ad. \n",
    "     ---- if the customer won't buy without ad --> we should've gave them the ad --> we are losing $(10 - 0.15) -->\n",
    "          (100% - 0.76%) of the people won't buy without ad\n",
    "     ---- if the customer will buy without ad then we are fine\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_control = y_pred_pd[y_pred_pd['Promotion'] == \"No\"]\n",
    "    y_pred_expr = y_pred_pd[y_pred_pd['Promotion'] == \"Yes\"]\n",
    "    \n",
    "    loss = 0.017 * (10 - 0.15) * sum((y_pred_control['purchase without ad'] == 1) & \\\n",
    "                                     (y_pred_control['purchase'] == 0)) + \\\n",
    "           0.15 * sum((y_pred_control['purchase without ad'] == 0) & \\\n",
    "                      (y_pred_control['purchase'] == 1) & \\\n",
    "                      (y_pred_control['purchase with ad'] == 1)) + \\\n",
    "           0.15 * sum((y_pred_expr['purchase with ad'] == 1) & \\\n",
    "                      (y_pred_expr['purchase'] == 0) & \\\n",
    "                      (y_pred_expr['purchase without ad'] == 0)) + \\\n",
    "           (1 - 0.0076) * (10 - 0.15) * sum((y_pred_expr['purchase with ad'] == 0) & \\\n",
    "                                            (y_pred_expr['purchase'] == 1)) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tune the model --- find the best parameters\n",
    "\n",
    "Parameters for us train include:\n",
    "\n",
    "`control_minority_group_size`: group size of the purchase control group after resampling\n",
    "<br>`control_majority_group_size`: group size of the non-purchase control group after resampling\n",
    "<br>`expr_minority_group_size`: group size of the purchase experiment group after resampling\n",
    "<br>`expr_majority_group_size`: group size of the non-purchase experiment group after resampling\n",
    "<br>`learning_rate`: parameter for GradientBoostingClassifier \n",
    "<br>`max_depth`: parameter for GradientBoostingClassifier \n",
    "<br>`min_samples_split`: parameter for GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Split our original data `train_data` to trainning set and testing set\n",
    "\n",
    "In each dataset, control group data and experiment data are mixed together because there's a column `Promotion` to indicate whether a record is in one group or another. Later in our functions we will be able to split them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_train, control_test = customize_train_test_split(train_data[train_data['Promotion'] == \"No\"], \"purchase\", 1)\n",
    "expr_train, expr_test = customize_train_test_split(train_data[train_data['Promotion'] == \"Yes\"], \"purchase\", 1)\n",
    "train = pd.concat([control_train, expr_train])\n",
    "test = pd.concat([control_test, expr_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Specify the candidate values for each variable and create a list of all combinations of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_minority_group_size = [1000, 1200]\n",
    "control_majority_group_size = [2000, 3000]\n",
    "expr_minority_group_size = [3000, 4000]\n",
    "expr_majority_group_size = [4000, 5000]\n",
    "learning_rate = [0.01, 0.02]\n",
    "max_depth = [6, 8]\n",
    "min_samples_split = [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(itertools.product(*[control_minority_group_size, \n",
    "                         control_majority_group_size,\n",
    "                         expr_minority_group_size,\n",
    "                         expr_majority_group_size,\n",
    "                         learning_rate,\n",
    "                         max_depth,\n",
    "                         min_samples_split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Gridsearch on the variables\n",
    "\n",
    "For each parameter combination, we run `generate_promotion_customer_list` 5 times and then calculate average loss. This is an approximate methods to 5 folds cross validation. Rather than spliting the data to 5 folds ahead and use one fold as validation set at one time, we randomly split data each time we run the model. This might be the laziness of the author :) and can be improved by 5 folds validation in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 2e+01 min\n",
      "Minimum average loss: 1080.3987359999999\n",
      "Best params : (1000, 3000, 4000, 4000, 0.01, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "begin = datetime.datetime.now()\n",
    "for p in params: \n",
    "    l = []\n",
    "    for i in range(5):\n",
    "        y_pred_pd = generate_promotion_customer_list(train,\n",
    "                                                     test,\n",
    "                                                     p[0],\n",
    "                                                     p[1],\n",
    "                                                     p[2],\n",
    "                                                     p[3],\n",
    "                                                     p[4],\n",
    "                                                     p[5])\n",
    "        l.append(evaluate(y_pred_pd))\n",
    "\n",
    "    loss_list.append(l)\n",
    "time_pass = (datetime.datetime.now() - begin).seconds / 60\n",
    "print(\"running time: {:.2} min\".format(time_pass))\n",
    "average_loss_list = list(np.mean(np.array(loss_list), axis=1))\n",
    "print(\"Minimum average loss: {}\".format(min(average_loss_list)))\n",
    "best_params = params[average_loss_list.index(min(average_loss_list))]\n",
    "print(\"Best params : {}\".format(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 The best parameters and minimum average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss_list = list(np.mean(np.array(loss_list), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080.3987359999999"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minimum average loss\n",
    "min(average_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.011428571428571429,\n",
       " 'recall': 0.019138755980861243,\n",
       " 'f1': 0.014311270125223612}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(train_model_full(train[train['Promotion'] == \"No\"], \n",
    "                     1200, \n",
    "                     2000, \n",
    "                     learning_rate=0.01, \n",
    "                     max_depth=6, \n",
    "                     min_samples_split=2),\n",
    "                test[['V1','V2','V3','V4','V5','V6','V7']],\n",
    "                test['purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.01706242001990616,\n",
       " 'recall': 0.5741626794258373,\n",
       " 'f1': 0.033140016570008285}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(train_model_full(train[train['Promotion'] == \"Yes\"], \n",
    "                     10000, \n",
    "                     10000, \n",
    "                     learning_rate=0.01, \n",
    "                     max_depth=6, \n",
    "                     min_samples_split=2),\n",
    "                test[['V1','V2','V3','V4','V5','V6','V7']],\n",
    "                test['purchase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the recommendation list (promotion strategy) and test it\n",
    "\n",
    "With the model above, we can predict who will buy with ad and who will without ad. We will give promotion ad to customers who won't buy with ad but will with ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_strategy(df):\n",
    "    \n",
    "    '''\n",
    "    INPUT \n",
    "    df - a dataframe with *only* the columns V1 - V7 (same as train_data)\n",
    "\n",
    "    OUTPUT\n",
    "    promotion_df - np.array with the values\n",
    "                   'Yes' or 'No' related to whether or not an \n",
    "                   individual should recieve a promotion \n",
    "                   should be the length of df.shape[0]\n",
    "                \n",
    "    Ex:\n",
    "    INPUT: df\n",
    "    \n",
    "    V1\tV2\t  V3\tV4\tV5\tV6\tV7\n",
    "    2\t30\t-1.1\t1\t1\t3\t2\n",
    "    3\t32\t-0.6\t2\t3\t2\t2\n",
    "    2\t30\t0.13\t1\t1\t4\t2\n",
    "    \n",
    "    OUTPUT: promotion\n",
    "    \n",
    "    array(['Yes', 'Yes', 'No'])\n",
    "    indicating the first two users would recieve the promotion and \n",
    "    the last should not.\n",
    "    '''\n",
    "    \n",
    "    y_pred_pd = generate_promotion_customer_list(train_data,\n",
    "                                     df,\n",
    "                                     best_params[0],\n",
    "                                     best_params[1],\n",
    "                                     best_params[2],\n",
    "                                     best_params[3],\n",
    "                                     learning_rate=best_params[4], \n",
    "                                     max_depth=best_params[5], \n",
    "                                     min_samples_split=best_params[6])\n",
    "    \n",
    "    result = np.array(((y_pred_pd['purchase without ad'] == 0) & (y_pred_pd['purchase with ad'] == 1))\\\n",
    "             .apply(lambda x: \"Yes\" if x else \"No\"))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  See how well your strategy worked on our test data below!\n",
      "\n",
      "Your irr with this strategy is 0.0195.\n",
      "\n",
      "Your nir with this strategy is 386.75.\n",
      "We came up with a model with an irr of 0.0188 and an nir of 189.45 on the test set.\n",
      "\n",
      " How did you do?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.019468787465423697, 386.75)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will test your results, and provide you back some information \n",
    "# on how well your promotion_strategy will work in practice\n",
    "\n",
    "test_results(promotion_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are a lot of resamplings (expecially down-samplings) and other processes that will generate different result under different `random_state`, we modified the function `test_results` to see the distributions of the 2 metrics under different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the model for 10 times\n",
      "Distributions of irr and nir:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_results_distributions(promotion_strategy, numbers_run=10):\n",
    "    print(\"Run the model for {} times\".format(numbers_run))\n",
    "    test_data = pd.read_csv('Test.csv')\n",
    "    df = test_data[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7']]\n",
    "    irrs = []\n",
    "    nirs = []\n",
    "    for i in range(numbers_run):\n",
    "        promos = promotion_strategy(df)\n",
    "        score_df = test_data.iloc[np.where(promos == 'Yes')]    \n",
    "        irr, nir = score(score_df)\n",
    "        irrs.append(irr)\n",
    "        nirs.append(nir)\n",
    "    print(\"Distributions of irr and nir:\")\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(irrs)\n",
    "    a1=plt.title('irr')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(nirs)\n",
    "    a1=plt.title('nir')\n",
    "\n",
    "test_results_distributions(promotion_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
